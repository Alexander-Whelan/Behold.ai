{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intracranial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcwQo3M4dNFn",
        "colab_type": "text"
      },
      "source": [
        "# Behold.ai Project\n",
        "Multi-Label Image Classification\n",
        "\n",
        "Data from RSNA Intracranial Hemorrhage Detection via Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0rzmNbCeVHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "53423e85-e7b6-47b3-e3e2-620767496b48"
      },
      "source": [
        "#Mount Google Drive folder where data is stored\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "!ls\n",
        "%cd gdrive/My Drive/Behold Project"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "gdrive\tsample_data\n",
            "/content/gdrive/My Drive/Behold Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWcz-iOgseEV",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wRRZTiydgC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from os import listdir\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Function for loading dataset\n",
        "def load_dataset(dataset_dir, is_train=True, csv_filename=''):\n",
        "  '''Loads each image into an array, normalises, and then appends to a list to create a machine-friendly dataset.\n",
        "  If train=True, it also loads the .csv file with target values. If test, it extracts filenames for the predictions.\n",
        "\n",
        "  Args = dataset_dir(str)\n",
        "        Where the dataset is stored(train or test)\n",
        "       = is_train(boolean, True by default)\n",
        "        Boolean switch to determine if a .csv file is read for given Y values\n",
        "       = csv_filename(str)\n",
        "        filename for the given Y values as a .csv\n",
        "\n",
        "  Returns = if is_train is true:\n",
        "            X (dataset of images as np array)\n",
        "            y (given target values as np array)\n",
        "          = if is_train is false:\n",
        "            X (dataset of images as np array)\n",
        "            filenames (list of filenames from dataset)\n",
        "  '''\n",
        "  if is_train:\n",
        "    data = pd.read_csv(csv_filename)\n",
        "    data.head()\n",
        "    \n",
        "    images = []\n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "      img = image.load_img(dataset_dir + '/' + data['ID'][i]+'.png')\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      images.append(img)\n",
        "    X = np.array(images)\n",
        "    y = np.array(data.drop(data.columns[[0, 1]], axis=1))\n",
        "    return X, y\n",
        "    \n",
        "  else:\n",
        "    images = []\n",
        "    filenames = []\n",
        "    for filename in tqdm(listdir(dataset_dir)):\n",
        "      img = image.load_img(dataset_dir + '/' + filename)\n",
        "      filenames.append(filename)\n",
        "      img = image.img_to_array(img)\n",
        "      img = img/255\n",
        "      images.append(img)\n",
        "    X = np.array(images)\n",
        "    return X, filenames"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jssFt7MLiF2i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f08f4ce1-19e5-4a1d-995f-04b59aa38371"
      },
      "source": [
        "#Load datasets, also check shapes\n",
        "[X, y] = load_dataset('train_images', is_train=True, csv_filename = 'behold_coding_challenge_train.csv')\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "[test_set, filenames] = load_dataset('test_images', is_train=False)\n",
        "print(test_set.shape)\n",
        "\n",
        "#Split training data into train set and a validation set to measure accuracy\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8542/8542 [00:11<00:00, 739.09it/s]\n",
            "  2%|▏         | 67/4019 [00:00<00:06, 650.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(8542, 128, 128, 3)\n",
            "(8542, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4019/4019 [00:04<00:00, 889.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(4019, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Yi5nKGsjD_",
        "colab_type": "text"
      },
      "source": [
        "# Model Building and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TA8EQKIsli4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "366c52bf-3c7e-4cc6-99e0-4e861e605da7"
      },
      "source": [
        "#Build a simple CNN model with 3 convolutional layers, \n",
        "#each layer with max pooling and dropout, and 3 dense layers\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\", input_shape=(128,128,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#Compile model with binary cross-entropy (as multi-label problem)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 124, 124, 16)      1216      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 62, 62, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 62, 62, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 58, 58, 32)        12832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 29, 29, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 29, 29, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 26912)             0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 64)                1722432   \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 1,736,675\n",
            "Trainable params: 1,736,675\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Ust1R3s7bn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "1a3ff858-0987-46bb-a2ba-02dc398e4818"
      },
      "source": [
        "#Model training\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), batch_size=16)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.4750 - accuracy: 0.3107 - val_loss: 0.4671 - val_accuracy: 0.3716\n",
            "Epoch 2/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.4503 - accuracy: 0.3348 - val_loss: 0.4415 - val_accuracy: 0.3534\n",
            "Epoch 3/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.4354 - accuracy: 0.3414 - val_loss: 0.4391 - val_accuracy: 0.3622\n",
            "Epoch 4/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.4245 - accuracy: 0.3609 - val_loss: 0.4433 - val_accuracy: 0.3441\n",
            "Epoch 5/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.4160 - accuracy: 0.3657 - val_loss: 0.4329 - val_accuracy: 0.3487\n",
            "Epoch 6/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.4068 - accuracy: 0.3647 - val_loss: 0.4373 - val_accuracy: 0.3640\n",
            "Epoch 7/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3942 - accuracy: 0.3717 - val_loss: 0.4331 - val_accuracy: 0.3563\n",
            "Epoch 8/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3820 - accuracy: 0.3774 - val_loss: 0.4353 - val_accuracy: 0.3528\n",
            "Epoch 9/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3691 - accuracy: 0.3878 - val_loss: 0.4373 - val_accuracy: 0.3640\n",
            "Epoch 10/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3612 - accuracy: 0.4013 - val_loss: 0.4430 - val_accuracy: 0.3552\n",
            "Epoch 11/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3460 - accuracy: 0.4035 - val_loss: 0.4469 - val_accuracy: 0.3622\n",
            "Epoch 12/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3366 - accuracy: 0.4172 - val_loss: 0.4705 - val_accuracy: 0.3640\n",
            "Epoch 13/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3288 - accuracy: 0.4241 - val_loss: 0.4762 - val_accuracy: 0.3581\n",
            "Epoch 14/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3223 - accuracy: 0.4262 - val_loss: 0.4905 - val_accuracy: 0.3441\n",
            "Epoch 15/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3101 - accuracy: 0.4306 - val_loss: 0.4718 - val_accuracy: 0.3376\n",
            "Epoch 16/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.3086 - accuracy: 0.4386 - val_loss: 0.4741 - val_accuracy: 0.3505\n",
            "Epoch 17/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.2944 - accuracy: 0.4603 - val_loss: 0.5392 - val_accuracy: 0.3359\n",
            "Epoch 18/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.2942 - accuracy: 0.4632 - val_loss: 0.4940 - val_accuracy: 0.3230\n",
            "Epoch 19/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.2854 - accuracy: 0.4619 - val_loss: 0.5232 - val_accuracy: 0.3423\n",
            "Epoch 20/20\n",
            "428/428 [==============================] - 3s 7ms/step - loss: 0.2793 - accuracy: 0.4550 - val_loss: 0.5275 - val_accuracy: 0.3370\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2608358fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rM5BO-bWCJm",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtvYF_cybyST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run some predictions on the test set\n",
        "def model_predictor(test_set, filenames, is_bool=True):\n",
        "  '''Predicts probabilities of each example having each label, converts to dataframe,\n",
        "  and then saves to a .csv file\n",
        "\n",
        "  Args = test_set(np array)\n",
        "        Test set of images\n",
        "       = filenames(list of str)\n",
        "        List of filenames from the test set so we can see which image has which prediction\n",
        "       = is_bool(boolean)\n",
        "        Boolean switch to toggle whether the user wants boolean (as in integer or binary)\n",
        "        predictions or probabilities (i.e. floats), and adjusts file name accordingly\n",
        "\n",
        "  Returns = Doesn't return anything, just saves the predictions as a .csv\n",
        "  '''\n",
        "    pred = model.predict(test_set)\n",
        "    if is_bool:\n",
        "      preds = (pred > 0.5)\n",
        "      predictions = preds.astype(int)\n",
        "    else:\n",
        "      predictions = pred.astype(float)\n",
        "    \n",
        "    #Sort out columns\n",
        "    columns=[\"epidural\", \"intraparenchymal\", \"subarachnoid\"]\n",
        "    results=pd.DataFrame(predictions, columns=columns)\n",
        "    results[\"Filenames\"]= filenames\n",
        "    ordered_cols=[\"Filenames\"] + columns\n",
        "    results=results[ordered_cols]\n",
        "\n",
        "    if is_bool:\n",
        "      results.to_csv(\"results_binary.csv\",index=False)\n",
        "    else: \n",
        "      results.to_csv(\"results_probabilities.csv\",index=False)\n",
        "\n",
        "model_predictor(test_set, filenames, is_bool=True)\n",
        "model_predictor(test_set, filenames, is_bool=False)"
      ],
      "execution_count": 60,
      "outputs": []
    }
  ]
}